{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02. Feature selection and GridSearch\n",
    "\n",
    "\n",
    "In this lab we will tackle two types of tasks: feature selection and hyperparameter tuning.\n",
    "\n",
    "Lectures and seminars you might find useful:\n",
    "- Lectures 1 - 4\n",
    "- Seminars 2 and 3\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "Each task has its value, **15 points** in total. If you use some open-source code please make sure to include the url.\n",
    "\n",
    "#### How to submit\n",
    "- Name your file according to this convention: `2022_lab02_GroupNumber_Surname_Name.ipynb`, for example \n",
    "    - `2022_lab02_404_Sheipak_Sviat.ipynb`\n",
    "    - `2022_lab02_NoGroup_Sheipak_Sviat.ipynb`\n",
    "- Attach your .ipynb to an email with topic `2022_lab02_GroupNumber_Surname_Name.ipynb`\n",
    "- Send it to `cosmic.research.ml@yandex.ru`\n",
    "- Deadline is ` 2022-10-20 23:00:00 +03:00`\n",
    "\n",
    "#### The Data:\n",
    "- All the datasets you need are here: https://disk.yandex.ru/d/vHEio0TmfAMmZg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Feature Selection [4 points]\n",
    "\n",
    "In this part of the assignemt you will be offered a task to analyze a dataset and figure out which features are the most important. The first means to solve this problem is to use linear model and examine the weights, another option is to train a logic classifier and see which featires it uses to build the splits. And finally you may use PCA and analyze how new PCA-features are configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Firstly, load the data from `feature_selection_sample.txt` and save it into variable `db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'feature_selection_sample.txt'\n",
    "db = pd.read_csv(input_filename, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature columns are `[0-9]` and the target is `[10]`. Split the table into object and target arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = # YOUR CODE HERE\n",
    "Y = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1 [1 point] Linear models**\n",
    "\n",
    "Import `LinearRegression` and define a problem with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and check the quality both on train set and test set. Since we are solving a regression problem, we will use `mean_squared_error` as a quality metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr.fit(# YOUR CODE HERE: fit the model)\n",
    "train_pred = # YOUR CODE HERE\n",
    "test_pred = # YOUR CODE HERE\n",
    "train_score = # YOUR CODE HERE\n",
    "test_score = # YOUR CODE HERE\n",
    "\n",
    "print(\"Linear Regression scores: train: {:.3f}, test: {:3.3f}\".format(train_score, test_score))\n",
    "original_test_score = test_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature-vector from the trained model (see [this page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)) and bar-plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coefs = # YOUR CODE HERE\n",
    "\n",
    "ncoef = model_coefs.shape[0]\n",
    "default_x = np.arange(ncoef)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(# YOUR CODE HERE)\n",
    "plt.xticks(default_x)\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Magnitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this plot, what are the most important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save 4 most important feature indexes to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature_idx = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these feature indexes to construct new train and test sets with smaller amount of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smaller = X_train[# YOUR CODE HERE]\n",
    "X_test_smaller = X_test[# YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new `LinearRegression` model, train and test it on new sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = # YOUR CODE HERE\n",
    "smaller_model.fit(# YOUR CODE HERE)\n",
    "train_pred = # YOUR CODE HERE\n",
    "test_pred = # YOUR CODE HERE\n",
    "train_score = # YOUR CODE HERE\n",
    "test_score = # YOUR CODE HERE\n",
    "print(\"{} train score: {:.3f}, test score: {:3.3f}\".format('Smaller LR', train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare scores of `model_lr` (variable `original_score`) and `smaller_model` (variable `smaller_test_score`). We reduced number of feature but why scores changes so drastically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2 [1 point] Linear models on scaled data**\n",
    "\n",
    "It is time to fix this failure and scale the data - we should have done it earlier, since we decided to use linear models. Import the scaler and apply it to all of the data (`X`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_scaler = # YOUR CODE HERE\n",
    "X_scaled = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat all the steps:\n",
    "- split the data\n",
    "- train a model on all features\n",
    "- plot coefficients\n",
    "- choose 4 most-important features\n",
    "- train a model on a feature-subset\n",
    "- compare the scores\n",
    "- profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc_train, X_sc_test, Y_sc_train, Y_sc_test = train_test_split(X_scaled, Y, test_size=0.1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_sc = # YOUR CODE HERE\n",
    "model_lr_sc.fit(# YOUR CODE HERE)\n",
    "train_pred = # YOUR CODE HERE\n",
    "test_pred = # YOUR CODE HERE\n",
    "train_score = # YOUR CODE HERE\n",
    "test_score = # YOUR CODE HERE\n",
    "print(\"Linear Regression on Scaled Data scores: train: {:.3f}, test: {:3.3f}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coefs = # YOUR CODE HERE\n",
    "\n",
    "ncoef = model_coefs.shape[0]\n",
    "default_x = np.arange(ncoef)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(default_x, model_coefs, label=model_name, width=0.5, color = 'red')\n",
    "plt.xticks(default_x)\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Magnitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important features now? Let's do the sanity check and train on this subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature_idx = [# YOUR CODE HERE] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc_train_smaller = X_sc_train[# YOUR CODE HERE]\n",
    "X_sc_test_smaller = X_sc_test[# YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_sc_smaller = # YOUR CODE HERE\n",
    "model_lr_sc_smaller.fit(# YOUR CODE HERE)\n",
    "train_pred = # YOUR CODE HERE\n",
    "test_pred = # YOUR CODE HERE\n",
    "train_score = # YOUR CODE HERE\n",
    "test_score = # YOUR CODE HERE\n",
    "print(\"Linear Regression on Scaled Data scores: train: {:.3f}, test: {:3.3f}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has **MSE** changed? To what extent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 [2 points] Decision Tree**\n",
    "\n",
    "As you probably now, there are models that are not influence by the fact that data is not normalized: for example, Decision Tree or Random Forest.\n",
    "\n",
    "Since you already have all the sets prepared: `X_train` and `X_sc_train`, train a RF model and prove that scaling does not affect feature importances.\n",
    "\n",
    "Then compare durations of training loops for a set with 10 features and 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_scaled = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(# YOUR CODE HERE)\n",
    "rf_scaled.fit(# YOUR CODE HERE)\n",
    "rf_test_score = # YOUR CODE HERE\n",
    "rf_scaled_test_score = # YOUR CODE HERE\n",
    "print(\"RF test score {:.3f}\".format(rf_test_score))\n",
    "print(\"RF scaled test score {:.3f}\".format(rf_scaled_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up an attribute for feature importances [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest#sklearn.ensemble.RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_coefs = # YOUR CODE HERE\n",
    "rf_scaled_model_coefs = # YOUR CODE HERE\n",
    "ncoef = rf_model_coefs.shape[0]\n",
    "default_x = np.arange(ncoef)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(default_x - 0.1, rf_model_coefs, label=\"RF\", width=0.1, color = 'red')\n",
    "plt.bar(default_x + 0.1, rf_scaled_model_coefs, label=\"RF Scaled\", width=0.1, color = 'blue')\n",
    "plt.xticks(default_x)\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Magnitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Comment on the plot:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now examine how reduction of number of features impacts durations of training loops. You may use `time` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of `time` usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "n = 1000\n",
    "a = np.diag(np.ones(n)) + np.random.rand(n, n)\n",
    "\n",
    "start = time()\n",
    "det = np.linalg.det(a)\n",
    "end = time()\n",
    "print(\"{} x {} matrix determinant took {:.3f} seconds\".format(n, n, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestRegressor()\n",
    "rf2 = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Comment on time consuption**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. GridSearch: hyperparameter tuning  [11 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will try to solve a multiclass classification task on Richter's dataset ([source](https://www.kaggle.com/mullerismail/richters-predictor-modeling-earthquake-damage)). The aim is to predict damage rate (label from 1 to 3).\n",
    "\n",
    "We will experiment with following models:\n",
    "- kNN\n",
    "- LinearRegression\n",
    "- DecisionTree\n",
    "- RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data, transform the table into arrays `X` and `y`, target column is called *damage_grade*. Note that objects are described with both numerical and categorical features. In the first part of this assignment we will use numerical features only (apply `_get_numeric_data()` to `pandas` dataframe).\n",
    "\n",
    "Split the data into `train`, `test` and `val` with ratio 4-to-2-to-1. Since we are going to use metric classifiers, don't forget to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"richters_sample.csv\")\n",
    "objects = data.drop(columns=\"damage_grade\")\n",
    "labels = data[\"damage_grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = # YOUR CODE HERE\n",
    "y = # YOUR CODE HERE\n",
    "\n",
    "assert X.shape == (35000, 31) and  y.shape == (35000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_not_train, y_train, y_not_train = train_test_split(X, y, test_size= #YOUR CODE HERE, \n",
    "                                                    shuffle=True, stratify= #YOUR CODE HERE,\n",
    "                                                    random_state=RANDOM_SEED)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_not_train, y_not_train, test_size= #YOUR CODE HERE, \n",
    "                                                    shuffle=True, stratify= #YOUR CODE HERE,\n",
    "                                                    random_state=RANDOM_SEED)\n",
    "\n",
    "assert X_train.shape[0] == 20000 and X_test.shape[0] == 10000 and X_val.shape[0] == 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import classification quality metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1 [2 points]. Default-parameter models**\n",
    "\n",
    "Let's take 4 classifiers (1 of a kind) with **default** parameters and check how well they can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = LogisticRegression()\n",
    "clf3 = DecisionTreeClassifier()\n",
    "clf4 = RandomForestClassifier()\n",
    "\n",
    "default_classifiers = [clf1, clf2, clf3, clf4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit each classifier on `X_train, y_train`, predict on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_predictions = []\n",
    "for clf in default_classifiers:\n",
    "    clf.fit(#YOUR CODE HERE)  #YOUR CODE HERE\n",
    "    pred = #YOUR CODE HERE\n",
    "    clf_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 5 metrics to each prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [accuracy_score(y_test, pred) for pred in clf_predictions]\n",
    "micro_precisions = [precision_score(y_test, pred, average=\"micro\", zero_division=1) for pred in clf_predictions] \n",
    "micro_recalls = #YOUR CODE HERE\n",
    "macro_precisions = #YOUR CODE HERE\n",
    "macro_recalls = #YOUR CODE HERE\n",
    "\n",
    "scores = [accuracies, micro_precisions, micro_recalls, macro_precisions, macro_recalls]\n",
    "names = [\"Accuracies\", \"Micro-Precisions\", \"Micro-Recalls\",  \"Macro-Precisions\", \"Macro-Recalls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 5), sharey=True)\n",
    "plt.setp(axs, ylim=(0, 1))\n",
    "\n",
    "xlabels = [\"knn\", \"linear\", \"dt\", \"rf\"]\n",
    "colors = [\"yellow\", \"red\", \"blue\", \"green\"]\n",
    "xticks = 1 + np.arange(len(xlabels))\n",
    "\n",
    "for ax, score, name in zip(axs, scores, names):\n",
    "    ax.bar(xticks, score, color=colors)\n",
    "    for i, v in enumerate(score):\n",
    "        ax.text(xticks[i] - 0.25, v + 0.01, \"{:.2f}\".format(v))\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_title(name)\n",
    "    ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model with the biggest gap between micro-precision and macro-precision and plot its confusion matrix.\n",
    "For confusion matrix do `from sklearn.metrics import confusion_matrix` (don't forget to put valid labels on plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakest_model_index = #YOUR CODE HERE: 0,1,2 or 3?\n",
    "\n",
    "dt_preds = clf_predictions[weakest_model_index]\n",
    "conf_matrix = confusion_matrix(#YOUR CODE HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels = sorted(labels.unique())\n",
    "\n",
    "sns.heatmap(#YOUR CODE HERE, \n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=#YOUR CODE HERE\n",
    "            yticklabels=#YOUR CODE HERE\n",
    "            linewidths=0.01, linecolor=\"black\", \n",
    "            annot = True, fmt='2g')\n",
    "\n",
    "plt.ylabel(\"GT\")\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going by confusion matrix, which class is the hardest to predict? How does it affect macro/micro-precision?\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2 [3 points]. 1-D Grid Search**\n",
    "\n",
    "No wonder that default models have scores far from perfect. Let's tweak those hyperparameters with GridSearch: we will iteratively look through all combinations of parameters in the grid and choose the best. At each iteraction use cross validation score with number of folds `k=5`.\n",
    "\n",
    "Firstly, build the grid for kNN. It will be a 1-D grid with the only parameter `n_neighbors`. Look through all values from 1 to 50.\n",
    "\n",
    "*Hint*: `np.arange`, `np.linspace` and `np.logspace` are very useful for grid constructions.\n",
    "\n",
    "**Attention** this part of assignment may need a lot of computational powers (as you probably remember, training of knn is quite expensive). \n",
    "\n",
    "To save some resources while doing grid search for knn you may use the trick from Part1: do feature-selection with DTree/RandomForest and select top-5 or top-10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "knn_grid = {\n",
    "    \"n_neighbors\": np.arange(#YOUR CODE HERE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_searcher = GridSearchCV(#YOUR CODE HERE, cv=5, return_train_score=True)\n",
    "knn_grid_searcher.fit(#YOUR CODE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mean cross-validation score for each hyperparameter:\n",
    "- X-axis is hyperparameter values\n",
    "- Y-axis is mean CV-score\n",
    "\n",
    "To show variance of obtained estimatets add *mean + 3 stds* and *mean - 3 stds* to the plot. You may use `plt.fill_between` to make it more descriptive (it will look like a coridor around the mean).\n",
    "\n",
    "To get values we need to plot search in `knn_grid_searcher` parameters and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = #YOUR CODE HERE \n",
    "stds = #YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "xs = knn_grid_searcher.param_grid[\"n_neighbors\"]\n",
    "plt.plot(#YOUR CODE HERE)\n",
    "plt.fill_between(#YOUR CODE HERE)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best estimator and its score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for 3 other models:\n",
    "- Decision Tree: grid search the `max_depth` parameter\n",
    "- LogisticRegression: `penalty`\n",
    "- RandomForest: `n_estimators`\n",
    "\n",
    "Some of the hyperparameters are not numeric, but categorical (like `penalty`) and you should choose some other way to plot cv-scores instead of `plt.plot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3 [3 points] 2-D Grid Search**\n",
    "\n",
    "Now it's time to improve the models with a 2-D grid search. For each classifier we will look for an optimal **pair** of hyperparameters. However, going through the whole grid may be computationally expensive, so here are some ways to speed it up:\n",
    "\n",
    "1. Make sparse grids with fewer number of parameters\n",
    "2. Choose random subsample from grid points and look for the optimum there\n",
    "3. Reduce number of folds in cross-validation\n",
    "4. Make a greedy grid search (use two grid-searchers sequentially)\n",
    "\n",
    "You have 4 models, 4 methods how to make grid search faster, choose one method per model and try it out.\n",
    "Report whether you got boost in quality.\n",
    "\n",
    "Here are default 2-D grids:\n",
    "- kNN:\n",
    "    - n_neighbors from 1 to 50\n",
    "    - metric: `euclidean`, `manhattan` or `chebyshev`\n",
    "    \n",
    "- linear\n",
    "    - penalty `l1`, `l2`, `elasticnet`, `none`\n",
    "    - C from 0.001 to 1000\n",
    "    \n",
    "- dtree:\n",
    "    - max_depth from 1 to 50\n",
    "    - criterion `gini` or `entropy`\n",
    "\n",
    "- rf\n",
    "    - n_estimators from 1 to 200\n",
    "    - max_features from 1 to 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4 [1 point] Categorical features**\n",
    "\n",
    "Add categorical features and examine how the influence performance of each model. Preprocess the data before applying a model: we need to encode categorical features with one-hot encoding (`get_dummies` from `pandas` or `OneHotEncoder` from `sklearn`).\n",
    "\n",
    "Don't forget to repeat the train-test-val splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_with_dummies = pd.get_dummies(#YOUR CODE HERE)\n",
    "\n",
    "X = #YOUR CODE HERE\n",
    "y = #YOUR CODE HERE\n",
    "\n",
    "assert X.shape == (35000, 69) and  y.shape == (35000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was your best model before adding categorical features?\n",
    "\n",
    "Use GridSearch + 5-fold CV on **train set** to define your new best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of which model increased the most? Why?\n",
    "\n",
    "**Your answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5 [2 point] Blending**\n",
    "\n",
    "Since you have already trained and tuned a lot of models, it might be useful to **blend** two best classifiers to get one even better.\n",
    "\n",
    "Pick two best models, say, `clf_a` and `clf_b`, train them on the `train_set`.\n",
    "\n",
    "Then use `Voting classifier` to build \n",
    "$$\n",
    "clf_c(\\alpha) = \\alpha \\cdot clf_a + (1 - \\alpha) \\cdot clf_b\n",
    "$$\n",
    "You will have to tune $\\alpha$ using grid search on `test_set` and then make final quality assessment on `val_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the best pair of models to blend? Did blending help to increase quality of each classifier?\n",
    "**Your answer here**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
